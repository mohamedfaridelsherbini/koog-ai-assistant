/*
 * This source file was generated by the Gradle 'init' task
 */
package dev.craftmind.agent

import kotlinx.coroutines.runBlocking
import kotlinx.serialization.Serializable
import kotlinx.serialization.json.buildJsonObject
import kotlinx.serialization.json.put
import kotlinx.serialization.json.putJsonArray
import kotlinx.serialization.json.addJsonObject
import java.net.http.HttpClient
import java.net.http.HttpRequest
import java.net.http.HttpResponse
import java.net.URI
import java.time.Duration

/**
 * Represents different Large Language Model providers.
 */
@Serializable
enum class LLMProvider {
    GOOGLE,
    OPENAI,
    META,
    ANTHROPIC,
    COHERE,
    MISTRAL,
    OLLAMA_DOCKER
}

/**
 * Represents different capabilities that an LLM can support.
 */
@Serializable
enum class LLMCapability {
    TEMPERATURE_ADJUSTMENT,
    TOOLS_USAGE,
    SCHEMA_BASED_TASKS,
    FUNCTION_CALLING,
    JSON_OUTPUT,
    STREAMING,
    MULTI_MODAL,
    CODE_GENERATION,
    TEXT_COMPLETION,
    CHAT_COMPLETION
}

/**
 * Docker-based Ollama AI Executor implementation
 */
class DockerOllamaExecutor(
    private val baseUrl: String = "http://localhost:11434",
    private var model: String = "llama3.1:8b",
    private val maxRetries: Int = 3,
    private val requestTimeout: Duration = Duration.ofMinutes(5),
    private val connectTimeout: Duration = Duration.ofSeconds(30)
) {
    private val httpClient = HttpClient.newBuilder()
        .connectTimeout(connectTimeout)
        .build()

    suspend fun execute(prompt: String, systemPrompt: String = "", conversationHistory: List<Map<String, String>> = emptyList()): String {
        var lastException: Exception? = null
        
        for (attempt in 1..maxRetries) {
            try {
                println("üîÑ Attempt $attempt/$maxRetries")
                
                // Build request with system prompt and conversation history
                val messages = mutableListOf<Map<String, String>>()
                if (systemPrompt.isNotEmpty()) {
                    messages.add(mapOf("role" to "system", "content" to systemPrompt))
                }
                messages.addAll(conversationHistory) // Add conversation history
                messages.add(mapOf("role" to "user", "content" to prompt))
                
                val requestBody = buildJsonObject {
                    put("model", model)
                    putJsonArray("messages") {
                        messages.forEach { message ->
                            addJsonObject {
                                put("role", message["role"] ?: "")
                                put("content", message["content"] ?: "")
                            }
                        }
                    }
                    put("stream", false)
                }.toString()

                println("üê≥ Docker Ollama: Sending request to $baseUrl/api/chat")
                println("üì§ Request body: $requestBody")
                println("üß† Context: ${conversationHistory.size} previous messages included")

                // Try Java HTTP client first
                var response: HttpResponse<String>? = null
                try {
                    val request = HttpRequest.newBuilder()
                        .uri(URI.create("$baseUrl/api/chat"))
                        .header("Content-Type", "application/json")
                        .POST(HttpRequest.BodyPublishers.ofString(requestBody))
                        .timeout(requestTimeout)
                        .build()

                    response = httpClient.send(request, HttpResponse.BodyHandlers.ofString())
                } catch (e: Exception) {
                    println("‚ö†Ô∏è Java HTTP client failed: ${e.message}")
                }
                
                // If Java HTTP client failed, try curl as fallback
                if (response == null || response.statusCode() != 200) {
                    println("üîÑ Trying curl fallback for chat...")
                    try {
                        val process = Runtime.getRuntime().exec(arrayOf("curl", "-s", "-X", "POST", "$baseUrl/api/chat", "-H", "Content-Type: application/json", "-d", requestBody))
                        val inputStream = process.inputStream
                        val responseBody = inputStream.bufferedReader().use { it.readText() }
                        process.waitFor()
                        
                        if (process.exitValue() == 0 && responseBody.isNotEmpty()) {
                            println("‚úÖ Curl fallback successful")
                            if (responseBody.contains("\"content\":")) {
                                val contentStart = responseBody.indexOf("\"content\":") + 11
                                val contentEnd = responseBody.indexOf("\"", contentStart)
                                if (contentEnd > contentStart) {
                                    val content = responseBody.substring(contentStart, contentEnd)
                                    return content
                                } else {
                                    return "Response received but content parsing failed"
                                }
                            } else {
                                return "Response received but no content found: $responseBody"
                            }
                        } else {
                            throw Exception("Curl failed with exit code: ${process.exitValue()}")
                        }
                    } catch (e: Exception) {
                        println("‚ùå Curl fallback failed: ${e.message}")
                        throw e
                    }
                } else {
                    println("üì• Response status: ${response.statusCode()}")
                    
                    if (response.statusCode() == 200) {
                        // Parse the Ollama response JSON
                        val responseBody = response.body()
                        if (responseBody.contains("\"content\":")) {
                            val contentStart = responseBody.indexOf("\"content\":") + 11
                            val contentEnd = responseBody.indexOf("\"", contentStart)
                            if (contentEnd > contentStart) {
                                val content = responseBody.substring(contentStart, contentEnd)
                                println("‚úÖ Success on attempt $attempt")
                                return content
                            } else {
                                return "Response received but content parsing failed"
                            }
                        } else {
                            return "Response received but no content found: $responseBody"
                        }
                    } else {
                        val errorMsg = "Error: HTTP ${response.statusCode()} - ${response.body()}"
                        println("‚ùå $errorMsg")
                        return errorMsg
                    }
                }
                
            } catch (e: Exception) {
                lastException = e
                val errorMsg = "Attempt $attempt failed: ${e.message}"
                println("‚ö†Ô∏è $errorMsg")
                
                if (attempt < maxRetries) {
                    val delayMs = (attempt * 1000).toLong() // Exponential backoff: 1s, 2s, 3s
                    println("‚è≥ Waiting ${delayMs}ms before retry...")
                    kotlinx.coroutines.delay(delayMs)
                }
            }
        }
        
        return "Error communicating with Docker Ollama after $maxRetries attempts. Last error: ${lastException?.message}"
    }
    
    suspend fun listAvailableModels(): String {
        return try {
            println("üîç Calling Ollama API at: $baseUrl/api/tags")
            
            // Try multiple times with different approaches
            for (attempt in 1..3) {
                try {
                    println("üîÑ Models attempt $attempt/3")
                    
                    // Create a fresh HTTP client with different settings
                    val tempHttpClient = HttpClient.newBuilder()
                        .connectTimeout(Duration.ofSeconds(10))
                        .build()
                        
                    val request = HttpRequest.newBuilder()
                        .uri(URI.create("$baseUrl/api/tags"))
                        .header("Accept", "application/json")
                        .header("User-Agent", "DockerAIAgent/1.0")
                        .GET()
                        .timeout(Duration.ofSeconds(10))
                        .build()

                    val response = tempHttpClient.send(request, HttpResponse.BodyHandlers.ofString())
                    
                    println("üìä Models API response status: ${response.statusCode()}")
                    println("üìã Models API response body: ${response.body()}")
                    
                    if (response.statusCode() == 200) {
                        val body = response.body()
                        // Check if we got actual models
                        if (body.contains("\"models\"") && !body.contains("\"models\":[]")) {
                            println("‚úÖ Success! Got models on attempt $attempt")
                            return body
                        } else {
                            println("‚ö†Ô∏è Got empty models list, retrying...")
                            if (attempt < 3) {
                                kotlinx.coroutines.delay(1000) // Wait 1 second before retry
                            }
                        }
                    } else {
                        println("‚ùå HTTP ${response.statusCode()}: ${response.body()}")
                        if (attempt < 3) {
                            kotlinx.coroutines.delay(1000)
                        }
                    }
                } catch (e: Exception) {
                    println("‚ùå Exception in attempt $attempt: ${e.message}")
                    if (attempt < 3) {
                        kotlinx.coroutines.delay(1000)
                    }
                }
            }
            
            // If all attempts failed, return a fallback
            println("‚ö†Ô∏è All attempts failed, returning fallback")
            """{"models":[{"name":"llama3.2:3b","model":"llama3.2:3b"}]}"""
            
        } catch (e: Exception) {
            println("‚ùå Exception in listAvailableModels: ${e.message}")
            e.printStackTrace()
            "Error listing models: ${e.message}"
        }
    }

    suspend fun pullModel(modelName: String): String {
        return try {
            // First try the normal API call
            val requestBody = """
                {
                    "name": "$modelName"
                }
            """.trimIndent()

            val request = HttpRequest.newBuilder()
                .uri(URI.create("$baseUrl/api/pull"))
                .header("Content-Type", "application/json")
                .POST(HttpRequest.BodyPublishers.ofString(requestBody))
                .timeout(Duration.ofMinutes(30)) // Longer timeout for model downloads
                .build()

            val response = httpClient.send(request, HttpResponse.BodyHandlers.ofString())
            
            if (response.statusCode() == 200) {
                // Parse streaming response to check for completion
                val responseBody = response.body()
                if (responseBody.contains("\"status\":\"success\"")) {
                    "Model $modelName pulled successfully"
                } else if (responseBody.contains("\"error\"")) {
                    // If there's an error, try using curl as fallback
                    println("üîÑ API call failed, trying curl fallback for model: $modelName")
                    return pullModelWithCurl(modelName)
                } else {
                    // Still in progress or completed without explicit success
                    "Model $modelName pulled successfully"
                }
            } else {
                // If HTTP call failed, try curl fallback
                println("üîÑ HTTP call failed, trying curl fallback for model: $modelName")
                return pullModelWithCurl(modelName)
            }
        } catch (e: Exception) {
            // If there's an exception, try curl fallback
            println("üîÑ Exception occurred, trying curl fallback for model: $modelName")
            return pullModelWithCurl(modelName)
        }
    }
    
    suspend fun pullModelWithProgress(modelName: String): String {
        return try {
            val requestBody = """
                {
                    "name": "$modelName"
                }
            """.trimIndent()

            val request = HttpRequest.newBuilder()
                .uri(URI.create("$baseUrl/api/pull"))
                .header("Content-Type", "application/json")
                .POST(HttpRequest.BodyPublishers.ofString(requestBody))
                .timeout(Duration.ofMinutes(30))
                .build()

            val response = httpClient.send(request, HttpResponse.BodyHandlers.ofString())
            
            if (response.statusCode() == 200) {
                val responseBody = response.body()
                println("üì• Pull response: $responseBody")
                
                // Parse the streaming response for progress updates
                val lines = responseBody.split("\n")
                var finalStatus = "unknown"
                
                for (line in lines) {
                    if (line.trim().isNotEmpty()) {
                        try {
                            // Simple JSON parsing for progress updates
                            if (line.contains("\"status\":\"downloading\"")) {
                                val completedMatch = Regex("\"completed\":(\\d+)").find(line)
                                val totalMatch = Regex("\"total\":(\\d+)").find(line)
                                
                                if (completedMatch != null && totalMatch != null) {
                                    val completed = completedMatch.groupValues[1].toLong()
                                    val total = totalMatch.groupValues[1].toLong()
                                    val progress = ((completed.toDouble() / total.toDouble()) * 100).toInt()
                                    
                                    println("üìä Progress: $progress% ($completed/$total)")
                                    
                                    // Send progress update as JSON
                                    val progressJson = """
                                        {"status": "downloading", "completed": $completed, "total": $total, "progress": $progress}
                                    """.trimIndent()
                                    println(progressJson)
                                }
                            } else if (line.contains("\"status\":\"success\"")) {
                                finalStatus = "success"
                            }
                        } catch (e: Exception) {
                            // Ignore non-JSON lines
                        }
                    }
                }
                
                if (finalStatus == "success" || responseBody.contains("pulled successfully") || responseBody.contains("already exists")) {
                    "Model $modelName pulled successfully"
                } else {
                    "Model $modelName pull completed with status: $responseBody"
                }
            } else {
                println("üîÑ API call failed, trying curl fallback for model: $modelName")
                pullModelWithCurl(modelName)
            }
        } catch (e: Exception) {
            println("üîÑ API call failed, trying curl fallback for model: $modelName")
            pullModelWithCurl(modelName)
        }
    }

    private suspend fun pullModelWithCurl(modelName: String): String {
        return try {
            println("üîÑ Using curl fallback to download model: $modelName")
            
            val processBuilder = ProcessBuilder(
                "curl", "-X", "POST",
                "http://localhost:11434/api/pull",
                "-H", "Content-Type: application/json",
                "-d", """{"name": "$modelName"}""",
                "--max-time", "1800" // 30 minutes timeout
            )
            
            val process = processBuilder.start()
            val exitCode = process.waitFor()
            
            if (exitCode == 0) {
                "Model $modelName pulled successfully using curl fallback"
            } else {
                "Error pulling model with curl: exit code $exitCode"
            }
        } catch (e: Exception) {
            "Error pulling model with curl fallback: ${e.message}"
        }
    }

    suspend fun deleteModel(modelName: String): String {
        return try {
            val requestBody = """
                {
                    "name": "$modelName"
                }
            """.trimIndent()

            val request = HttpRequest.newBuilder()
                .uri(URI.create("$baseUrl/api/delete"))
                .header("Content-Type", "application/json")
                .method("DELETE", HttpRequest.BodyPublishers.ofString(requestBody))
                .timeout(Duration.ofSeconds(30))
                .build()

            val response = httpClient.send(request, HttpResponse.BodyHandlers.ofString())
            
            if (response.statusCode() == 200) {
                "Model $modelName deleted successfully"
            } else {
                "Error deleting model: HTTP ${response.statusCode()} - ${response.body()}"
            }
        } catch (e: Exception) {
            "Error deleting model: ${e.message}"
        }
    }

    suspend fun switchModel(newModel: String): String {
        return try {
            // Try to get models using curl fallback (same as web server)
            var modelsResponse = ""
            try {
                val process = Runtime.getRuntime().exec(arrayOf("curl", "-s", "$baseUrl/api/tags"))
                val inputStream = process.inputStream
                modelsResponse = inputStream.bufferedReader().use { it.readText() }
                process.waitFor()
                
                if (process.exitValue() != 0) {
                    throw Exception("Curl failed with exit code: ${process.exitValue()}")
                }
            } catch (e: Exception) {
                println("‚ùå Curl fallback failed: ${e.message}")
                // Fallback to Java HTTP client
                modelsResponse = listAvailableModels()
            }
            
            if (modelsResponse.contains(newModel)) {
                model = newModel
                "Model switched to $newModel successfully"
            } else {
                // Try to extract model names from the response
                val modelNames = try {
                    val jsonResponse = modelsResponse
                    if (jsonResponse.contains("\"name\":")) {
                        val names = mutableListOf<String>()
                        val namePattern = "\"name\":\"([^\"]+)\"".toRegex()
                        namePattern.findAll(jsonResponse).forEach { matchResult ->
                            names.add(matchResult.groupValues[1])
                        }
                        names
                    } else {
                        emptyList()
                    }
                } catch (e: Exception) {
                    emptyList()
                }
                
                if (modelNames.isNotEmpty()) {
                    "Error: Model $newModel not found. Available models: ${modelNames.joinToString(", ")}"
                } else {
                    "Error: Model $newModel not found. Available models: $modelsResponse"
                }
            }
        } catch (e: Exception) {
            "Error switching model: ${e.message}"
        }
    }
    
    suspend fun checkHealth(): String {
        return try {
            val request = HttpRequest.newBuilder()
                .uri(URI.create("$baseUrl/api/tags"))
                .GET()
                .timeout(Duration.ofSeconds(10))
                .build()

            val response = httpClient.send(request, HttpResponse.BodyHandlers.ofString())
            
            if (response.statusCode() == 200) {
                "Healthy - Ollama is responding"
            } else {
                "Unhealthy - HTTP ${response.statusCode()}"
            }
        } catch (e: Exception) {
            "Unhealthy - ${e.message}"
        }
    }

    fun getCurrentModel(): String = model
    
    suspend fun getAllAvailableModels(): String {
        // For now, always use the fallback list since the registry API might not be accessible
        println("üîç Using fallback model list...")
        return getFallbackModelList()
    }
    
    private fun getFallbackModelList(): String {
        // Return a curated list of popular models as fallback
        return """
        {
            "models": [
                {"name": "llama3.1:8b", "size": "4.6GB", "parameterSize": "8B", "quantizationLevel": "Q4_K_M"},
                {"name": "llama3.1:70b", "size": "39GB", "parameterSize": "70B", "quantizationLevel": "Q4_K_M"},
                {"name": "llama3.2:1b", "size": "1.2GB", "parameterSize": "1.2B", "quantizationLevel": "Q8_0"},
                {"name": "llama3.2:3b", "size": "1.9GB", "parameterSize": "3.2B", "quantizationLevel": "Q4_K_M"},
                {"name": "llama3.2:11b", "size": "6.2GB", "parameterSize": "11B", "quantizationLevel": "Q4_K_M"},
                {"name": "llama3.2:90b", "size": "50GB", "parameterSize": "90B", "quantizationLevel": "Q4_K_M"},
                {"name": "codellama:7b", "size": "3.6GB", "parameterSize": "7B", "quantizationLevel": "Q4_0"},
                {"name": "codellama:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_0"},
                {"name": "codellama:34b", "size": "19GB", "parameterSize": "34B", "quantizationLevel": "Q4_0"},
                {"name": "mistral:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "mixtral:8x7b", "size": "45GB", "parameterSize": "8x7B", "quantizationLevel": "Q4_K_M"},
                {"name": "neural-chat:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "starling-lm:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "openchat:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "gemma:2b", "size": "1.6GB", "parameterSize": "2B", "quantizationLevel": "Q4_K_M"},
                {"name": "gemma:7b", "size": "4.8GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "phi3:mini", "size": "2.3GB", "parameterSize": "3.8B", "quantizationLevel": "Q4_K_M"},
                {"name": "phi3:medium", "size": "7.4GB", "parameterSize": "14B", "quantizationLevel": "Q4_K_M"},
                {"name": "qwen2.5:7b", "size": "4.4GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "qwen2.5:14b", "size": "8.6GB", "parameterSize": "14B", "quantizationLevel": "Q4_K_M"},
                {"name": "qwen2.5:32b", "size": "19GB", "parameterSize": "32B", "quantizationLevel": "Q4_K_M"},
                {"name": "deepseek-coder:6.7b", "size": "3.8GB", "parameterSize": "6.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "deepseek-coder:33b", "size": "19GB", "parameterSize": "33B", "quantizationLevel": "Q4_K_M"},
                {"name": "wizardcoder:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "wizardcoder:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_K_M"},
                {"name": "wizardcoder:34b", "size": "19GB", "parameterSize": "34B", "quantizationLevel": "Q4_K_M"},
                {"name": "llama2:7b", "size": "3.8GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "llama2:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_K_M"},
                {"name": "llama2:70b", "size": "39GB", "parameterSize": "70B", "quantizationLevel": "Q4_K_M"},
                {"name": "orca-mini:3b", "size": "1.9GB", "parameterSize": "3B", "quantizationLevel": "Q4_K_M"},
                {"name": "orca-mini:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "dolphin-2.6-mistral:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "dolphin-2.7-mixtral:8x7b", "size": "45GB", "parameterSize": "8x7B", "quantizationLevel": "Q4_K_M"},
                {"name": "zephyr:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "zephyr:14b", "size": "8.6GB", "parameterSize": "14B", "quantizationLevel": "Q4_K_M"},
                {"name": "vicuna:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "vicuna:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_K_M"},
                {"name": "alpaca:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "alpaca:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_K_M"},
                {"name": "wizard-vicuna:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "wizard-vicuna:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_K_M"},
                {"name": "wizard-vicuna:30b", "size": "19GB", "parameterSize": "30B", "quantizationLevel": "Q4_K_M"},
                {"name": "wizard-mega:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_K_M"},
                {"name": "wizard-mega:30b", "size": "19GB", "parameterSize": "30B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes:70b", "size": "39GB", "parameterSize": "70B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2:7b", "size": "4.1GB", "parameterSize": "7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2:13b", "size": "7.3GB", "parameterSize": "13B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2:70b", "size": "39GB", "parameterSize": "70B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-mixtral:8x7b", "size": "45GB", "parameterSize": "8x7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.1", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.2", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.3", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.4", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.5", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.6", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.7", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.8", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v0.9", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"},
                {"name": "nous-hermes2-solar:10.7b-instruct-v1.0", "size": "6.2GB", "parameterSize": "10.7B", "quantizationLevel": "Q4_K_M"}
            ]
        }
        """.trimIndent()
    }
}

/**
 * AI Agent implementation using Docker Ollama with conversation memory and file I/O
 */
class DockerAIAgent(
    private val systemPrompt: String,
    private val modelName: String,
    private val executor: DockerOllamaExecutor,
    private val maxMemorySize: Int = 10 // Keep last 10 messages for context
) {
    private val conversationMemory = mutableListOf<Pair<String, String>>() // (user, assistant) pairs
    private val conversationHistory = mutableListOf<ConversationEntry>()
    private var totalTokens = 0L
    private var sessionStartTime = System.currentTimeMillis()
    private var cachedModels: String? = null
    private var lastCacheTime = 0L
    private var systemMetrics = SystemMetrics()
    
    @Serializable
    data class ConversationEntry(
        val timestamp: Long,
        val userMessage: String,
        val assistantMessage: String,
        val modelUsed: String,
        val responseTime: Long
    )
    
    @Serializable
    data class SystemMetrics(
        val cpuUsage: Double = 0.0,
        val memoryUsage: Double = 0.0,
        val diskUsage: Double = 0.0,
        val networkLatency: Long = 0L,
        val activeConnections: Int = 0,
        val uptime: Long = 0L,
        val lastUpdated: Long = System.currentTimeMillis()
    )
    
    @Serializable
    data class PerformanceMetrics(
        val averageResponseTime: Double,
        val totalRequests: Int,
        val successfulRequests: Int,
        val failedRequests: Int,
        val throughput: Double, // requests per minute
        val errorRate: Double,
        val peakResponseTime: Long,
        val minResponseTime: Long
    )
    
    suspend fun run(prompt: String): String {
        val startTime = System.currentTimeMillis()
        
        // Update system metrics before processing
        updateSystemMetrics()
        
        // Add user message to memory
        conversationMemory.add(prompt to "")
        
        // Build conversation history for context
        val conversationHistory = buildConversationHistory()
        
        // Execute with full context
        val response = executor.execute(prompt, systemPrompt, conversationHistory)
        
        val endTime = System.currentTimeMillis()
        val responseTime = endTime - startTime
        
        // Update the last entry with the assistant's response
        if (conversationMemory.isNotEmpty()) {
            val lastIndex = conversationMemory.size - 1
            // Only store successful responses, not error messages
            if (!response.startsWith("Error:")) {
                conversationMemory[lastIndex] = conversationMemory[lastIndex].first to response
            } else {
                // Remove the failed conversation entry
                conversationMemory.removeAt(lastIndex)
            }
        }
        
        // Add to conversation history
        this.conversationHistory.add(
            ConversationEntry(
                timestamp = startTime,
                userMessage = prompt,
                assistantMessage = response,
                modelUsed = executor.getCurrentModel(),
                responseTime = responseTime
            )
        )
        
        // Maintain memory size limit
        if (conversationMemory.size > maxMemorySize) {
            conversationMemory.removeAt(0)
        }
        
        return response
    }
    
    private fun buildConversationHistory(): List<Map<String, String>> {
        val history = mutableListOf<Map<String, String>>()
        
        for ((userMsg, assistantMsg) in conversationMemory.dropLast(1)) { // Exclude current message
            if (userMsg.isNotEmpty()) {
                history.add(mapOf("role" to "user", "content" to userMsg))
            }
            // Only include assistant messages that are not error messages
            if (assistantMsg.isNotEmpty() && !assistantMsg.startsWith("Error:")) {
                history.add(mapOf("role" to "assistant", "content" to assistantMsg))
            }
        }
        
        return history
    }
    
    fun getMemorySize(): Int = conversationMemory.size
    
    fun getMemorySummary(): String {
        return when {
            conversationMemory.isEmpty() -> "No conversation history"
            conversationMemory.size == 1 -> "1 message in memory"
            else -> "${conversationMemory.size} messages in memory"
        }
    }
    
    fun clearMemory() {
        conversationMemory.clear()
    }
    
    // System Monitoring Functions
    
    private fun updateSystemMetrics() {
        try {
            val runtime = Runtime.getRuntime()
            val totalMemory = runtime.totalMemory()
            val freeMemory = runtime.freeMemory()
            val usedMemory = totalMemory - freeMemory
            val memoryUsagePercent = (usedMemory.toDouble() / totalMemory.toDouble()) * 100
            
            // Get CPU usage (simplified - in production you'd use a more sophisticated method)
            val cpuUsage = getCpuUsage()
            
            // Get disk usage
            val diskUsage = getDiskUsage()
            
            // Get network latency to Ollama
            val networkLatency = getNetworkLatency()
            
            systemMetrics = SystemMetrics(
                cpuUsage = cpuUsage,
                memoryUsage = memoryUsagePercent,
                diskUsage = diskUsage,
                networkLatency = networkLatency,
                activeConnections = getActiveConnections(),
                uptime = System.currentTimeMillis() - sessionStartTime,
                lastUpdated = System.currentTimeMillis()
            )
        } catch (e: Exception) {
            println("‚ö†Ô∏è Error updating system metrics: ${e.message}")
        }
    }
    
    private fun getCpuUsage(): Double {
        return try {
            // Simplified CPU usage calculation
            // In production, you'd use a more sophisticated method like SIGAR or OSHI
            val runtime = Runtime.getRuntime()
            val availableProcessors = runtime.availableProcessors()
            val load = Math.random() * 100 // Placeholder - replace with actual CPU monitoring
            load.coerceIn(0.0, 100.0)
        } catch (e: Exception) {
            0.0
        }
    }
    
    private fun getDiskUsage(): Double {
        return try {
            val file = java.io.File(".")
            val totalSpace = file.totalSpace
            val freeSpace = file.freeSpace
            val usedSpace = totalSpace - freeSpace
            (usedSpace.toDouble() / totalSpace.toDouble()) * 100
        } catch (e: Exception) {
            0.0
        }
    }
    
    private fun getNetworkLatency(): Long {
        return try {
            val startTime = System.currentTimeMillis()
            val request = HttpRequest.newBuilder()
                .uri(URI.create("http://localhost:11434/api/tags"))
                .GET()
                .timeout(Duration.ofSeconds(5))
                .build()
            
            val httpClient = HttpClient.newBuilder().build()
            httpClient.send(request, HttpResponse.BodyHandlers.ofString())
            
            System.currentTimeMillis() - startTime
        } catch (e: Exception) {
            -1L
        }
    }
    
    private fun getActiveConnections(): Int {
        return try {
            // Simplified connection count
            // In production, you'd track actual HTTP connections
            conversationHistory.size
        } catch (e: Exception) {
            0
        }
    }
    
    fun getSystemMetrics(): SystemMetrics {
        updateSystemMetrics()
        return systemMetrics
    }
    
    fun getPerformanceMetrics(): PerformanceMetrics {
        if (conversationHistory.isEmpty()) {
            return PerformanceMetrics(
                averageResponseTime = 0.0,
                totalRequests = 0,
                successfulRequests = 0,
                failedRequests = 0,
                throughput = 0.0,
                errorRate = 0.0,
                peakResponseTime = 0L,
                minResponseTime = 0L
            )
        }
        
        val responseTimes = conversationHistory.map { it.responseTime }
        val successfulRequests = conversationHistory.count { it.assistantMessage.isNotEmpty() }
        val failedRequests = conversationHistory.size - successfulRequests
        val sessionDurationMinutes = (System.currentTimeMillis() - sessionStartTime) / 60000.0
        
        return PerformanceMetrics(
            averageResponseTime = responseTimes.average(),
            totalRequests = conversationHistory.size,
            successfulRequests = successfulRequests,
            failedRequests = failedRequests,
            throughput = if (sessionDurationMinutes > 0) conversationHistory.size / sessionDurationMinutes else 0.0,
            errorRate = if (conversationHistory.size > 0) (failedRequests.toDouble() / conversationHistory.size) * 100 else 0.0,
            peakResponseTime = responseTimes.maxOrNull() ?: 0L,
            minResponseTime = responseTimes.minOrNull() ?: 0L
        )
    }
    
    fun getSystemHealth(): Map<String, Any> {
        val metrics = getSystemMetrics()
        val performance = getPerformanceMetrics()
        
        val healthStatus = when {
            metrics.cpuUsage > 90 -> "CRITICAL"
            metrics.cpuUsage > 80 -> "WARNING"
            metrics.memoryUsage > 90 -> "CRITICAL"
            metrics.memoryUsage > 80 -> "WARNING"
            performance.errorRate > 10 -> "WARNING"
            else -> "HEALTHY"
        }
        
        return mapOf(
            "status" to healthStatus,
            "systemMetrics" to metrics,
            "performanceMetrics" to performance,
            "recommendations" to getHealthRecommendations(metrics, performance)
        )
    }
    
    private fun getHealthRecommendations(metrics: SystemMetrics, performance: PerformanceMetrics): List<String> {
        val recommendations = mutableListOf<String>()
        
        if (metrics.cpuUsage > 80) {
            recommendations.add("High CPU usage detected. Consider reducing concurrent requests or upgrading resources.")
        }
        
        if (metrics.memoryUsage > 80) {
            recommendations.add("High memory usage detected. Consider clearing conversation history or increasing memory allocation.")
        }
        
        if (metrics.diskUsage > 90) {
            recommendations.add("High disk usage detected. Consider cleaning up temporary files or increasing disk space.")
        }
        
        if (performance.errorRate > 5) {
            recommendations.add("High error rate detected. Check network connectivity and Ollama service status.")
        }
        
        if (performance.averageResponseTime > 30000) {
            recommendations.add("Slow response times detected. Consider using a faster model or optimizing requests.")
        }
        
        if (recommendations.isEmpty()) {
            recommendations.add("System is performing well. No immediate actions required.")
        }
        
        return recommendations
    }
    
    // Advanced Conversation Features
    
    fun getConversationStats(): Map<String, Any> {
        val sessionDuration = System.currentTimeMillis() - sessionStartTime
        val avgResponseTime = if (conversationHistory.isNotEmpty()) {
            conversationHistory.map { it.responseTime }.average()
        } else 0.0
        
        return mapOf(
            "totalMessages" to conversationHistory.size,
            "sessionDuration" to sessionDuration,
            "averageResponseTime" to avgResponseTime,
            "currentModel" to executor.getCurrentModel(),
            "memorySize" to conversationMemory.size,
            "sessionStartTime" to sessionStartTime
        )
    }
    
    fun exportConversation(format: String = "json"): String {
        return when (format.lowercase()) {
            "json" -> exportConversationJson()
            "txt" -> exportConversationTxt()
            "csv" -> exportConversationCsv()
            else -> "Unsupported format: $format"
        }
    }
    
    private fun exportConversationJson(): String {
        val exportData = mapOf(
            "metadata" to mapOf(
                "exportTime" to System.currentTimeMillis(),
                "totalMessages" to conversationHistory.size,
                "currentModel" to executor.getCurrentModel(),
                "sessionDuration" to (System.currentTimeMillis() - sessionStartTime)
            ),
            "conversations" to conversationHistory
        )
        
        return kotlinx.serialization.json.Json.encodeToString(exportData.toString())
    }
    
    private fun exportConversationTxt(): String {
        return buildString {
            appendLine("=== Docker AI Agent Conversation Export ===")
            appendLine("Export Time: ${java.time.LocalDateTime.now()}")
            appendLine("Total Messages: ${conversationHistory.size}")
            appendLine("Current Model: ${executor.getCurrentModel()}")
            appendLine("Session Duration: ${(System.currentTimeMillis() - sessionStartTime) / 1000}s")
            appendLine("=".repeat(50))
            appendLine()
            
            conversationHistory.forEachIndexed { index, entry ->
                appendLine("Message #${index + 1}")
                appendLine("Time: ${java.time.Instant.ofEpochMilli(entry.timestamp)}")
                appendLine("Model: ${entry.modelUsed}")
                appendLine("Response Time: ${entry.responseTime}ms")
                appendLine("üë§ User: ${entry.userMessage}")
                appendLine("ü§ñ AI: ${entry.assistantMessage}")
                appendLine("-".repeat(30))
                appendLine()
            }
        }
    }
    
    private fun exportConversationCsv(): String {
        return buildString {
            appendLine("Timestamp,Model,ResponseTime,UserMessage,AssistantMessage")
            conversationHistory.forEach { entry ->
                appendLine("${entry.timestamp},${entry.modelUsed},${entry.responseTime},\"${entry.userMessage.replace("\"", "\"\"")}\",\"${entry.assistantMessage.replace("\"", "\"\"")}\"")
            }
        }
    }
    
    fun importConversation(jsonData: String): String {
        return try {
            // Simple import - just parse and validate
            val data = kotlinx.serialization.json.Json.decodeFromString<Map<String, Any>>(jsonData)
            val conversations = data["conversations"] as? List<Map<String, Any>>
            
            if (conversations != null) {
                conversationHistory.clear()
                conversations.forEach { conv ->
                    conversationHistory.add(
                        ConversationEntry(
                            timestamp = (conv["timestamp"] as? Number)?.toLong() ?: System.currentTimeMillis(),
                            userMessage = conv["userMessage"] as? String ?: "",
                            assistantMessage = conv["assistantMessage"] as? String ?: "",
                            modelUsed = conv["modelUsed"] as? String ?: executor.getCurrentModel(),
                            responseTime = (conv["responseTime"] as? Number)?.toLong() ?: 0L
                        )
                    )
                }
                "Successfully imported ${conversationHistory.size} conversation entries"
            } else {
                "Invalid conversation data format"
            }
        } catch (e: Exception) {
            "Error importing conversation: ${e.message}"
        }
    }
    
    fun getConversationAnalytics(): Map<String, Any> {
        if (conversationHistory.isEmpty()) {
            return mapOf("message" to "No conversation data available")
        }
        
        val responseTimes = conversationHistory.map { it.responseTime }
        val modelsUsed = conversationHistory.map { it.modelUsed }.distinct()
        val messageLengths = conversationHistory.map { it.userMessage.length + it.assistantMessage.length }
        
        return mapOf(
            "totalMessages" to conversationHistory.size,
            "uniqueModels" to modelsUsed.size,
            "modelsUsed" to modelsUsed,
            "averageResponseTime" to responseTimes.average(),
            "minResponseTime" to (responseTimes.minOrNull() ?: 0),
            "maxResponseTime" to (responseTimes.maxOrNull() ?: 0),
            "averageMessageLength" to messageLengths.average(),
            "totalCharacters" to messageLengths.sum(),
            "sessionDuration" to (System.currentTimeMillis() - sessionStartTime) / 1000.0
        )
    }
    
    fun resetSession() {
        conversationMemory.clear()
        conversationHistory.clear()
        sessionStartTime = System.currentTimeMillis()
        totalTokens = 0L
    }
    
    // File I/O Functions
    fun saveConversation(filename: String = "conversation_${System.currentTimeMillis()}.txt"): String {
        return try {
            val file = java.io.File(filename)
            val content = buildString {
                appendLine("=== Docker AI Agent Conversation ===")
                appendLine("Date: ${java.time.LocalDateTime.now()}")
                appendLine("Model: $modelName")
                appendLine("Total Messages: ${conversationHistory.size}")
                appendLine("=".repeat(50))
                appendLine()
                
                conversationHistory.forEachIndexed { index, entry ->
                    appendLine("Message #${index + 1}")
                    appendLine("Time: ${java.time.Instant.ofEpochMilli(entry.timestamp)}")
                    appendLine("Model: ${entry.modelUsed}")
                    appendLine("Response Time: ${entry.responseTime}ms")
                    appendLine("üë§ User: ${entry.userMessage}")
                    appendLine("ü§ñ AI: ${entry.assistantMessage}")
                    appendLine("-".repeat(30))
                    appendLine()
                }
                
                appendLine("=== End of Conversation ===")
            }
            
            file.writeText(content)
            "‚úÖ Conversation saved to: $filename"
        } catch (e: Exception) {
            "‚ùå Error saving conversation: ${e.message}"
        }
    }
    
    fun readFile(filename: String): String {
        return try {
            val file = java.io.File(filename)
            if (!file.exists()) {
                return "‚ùå File not found: $filename"
            }
            if (!file.canRead()) {
                return "‚ùå Cannot read file: $filename"
            }
            
            val content = file.readText()
            "üìÑ File content from '$filename':\n$content"
        } catch (e: Exception) {
            "‚ùå Error reading file: ${e.message}"
        }
    }
    
    fun writeFile(filename: String, content: String): String {
        return try {
            val file = java.io.File(filename)
            file.writeText(content)
            "‚úÖ File written successfully: $filename"
        } catch (e: Exception) {
            "‚ùå Error writing file: ${e.message}"
        }
    }
    
    fun listFiles(directory: String = "."): String {
        return try {
            val dir = java.io.File(directory)
            if (!dir.exists() || !dir.isDirectory) {
                return "‚ùå Directory not found: $directory"
            }
            
            val files = dir.listFiles()?.filter { it.isFile }?.map { it.name } ?: emptyList()
            val dirs = dir.listFiles()?.filter { it.isDirectory }?.map { it.name } ?: emptyList()
            
            buildString {
                appendLine("üìÅ Directory: $directory")
                appendLine("üìÑ Files (${files.size}):")
                files.forEach { appendLine("  ‚Ä¢ $it") }
                appendLine("üìÅ Directories (${dirs.size}):")
                dirs.forEach { appendLine("  ‚Ä¢ $it/") }
            }
        } catch (e: Exception) {
            "‚ùå Error listing directory: ${e.message}"
        }
    }
    
    // Model Management Methods
    suspend fun listModels(): String {
        return executor.listAvailableModels()
    }
    
    suspend fun pullModel(modelName: String): String {
        return executor.pullModel(modelName)
    }
    
    suspend fun pullModelWithProgress(modelName: String): String {
        return executor.pullModelWithProgress(modelName)
    }
    
    suspend fun deleteModel(modelName: String): String {
        return executor.deleteModel(modelName)
    }
    
    suspend fun switchModel(newModel: String): String {
        return executor.switchModel(newModel)
    }
    
    suspend fun checkHealth(): String {
        return executor.checkHealth()
    }
    
    fun getCurrentModel(): String = executor.getCurrentModel()
    
    suspend fun getAllModelsWithStatus(): String {
        return try {
            val currentTime = System.currentTimeMillis()
            
            // Use cache if it's less than 30 seconds old
            if (cachedModels != null && (currentTime - lastCacheTime) < 30000) {
                println("üîç Using cached models data")
                return cachedModels!!
            }
            
            // Get downloaded models
            val downloadedModelsJson = executor.listAvailableModels()
            val downloadedModels = parseDownloadedModels(downloadedModelsJson)
            println("üîç Found ${downloadedModels.size} downloaded models")
            
            // Get all available models from registry (cached)
            val allModels = parseAllModels("") // Use hardcoded list to avoid API calls
            println("üîç Using cached list of ${allModels.size} available models")
            
            // Create combined response
            val response = buildJsonObject {
                putJsonArray("downloadedModels") {
                    downloadedModels.forEach { model ->
                        addJsonObject {
                            put("name", model.name)
                            put("size", model.size)
                            put("parameterSize", model.parameterSize)
                            put("quantizationLevel", model.quantizationLevel)
                            put("isDownloaded", true)
                            put("downloadSize", model.downloadSize)
                        }
                    }
                }
                putJsonArray("availableModels") {
                    allModels.forEach { model ->
                        val isDownloaded = downloadedModels.any { it.name == model.name }
                        // Only add to available models if it's not downloaded
                        if (!isDownloaded) {
                            addJsonObject {
                                put("name", model.name)
                                put("size", model.size)
                                put("parameterSize", model.parameterSize)
                                put("quantizationLevel", model.quantizationLevel)
                                put("isDownloaded", false)
                                put("downloadSize", "")
                            }
                        }
                    }
                }
                put("status", "success")
            }
            
            val result = response.toString()
            
            // Cache the result
            cachedModels = result
            lastCacheTime = currentTime
            
            result
        } catch (e: Exception) {
            println("‚ùå Error in getAllModelsWithStatus: ${e.message}")
            e.printStackTrace()
            """{"downloadedModels":[],"availableModels":[],"status":"error","message":"${e.message}"}"""
        }
    }
    
    private fun parseDownloadedModels(json: String): List<ModelInfo> {
        return try {
            val models = mutableListOf<ModelInfo>()
            
            // Use regex to find all model objects
            val modelPattern = "\"name\"\\s*:\\s*\"([^\"]+)\".*?\"size\"\\s*:\\s*(\\d+).*?\"parameter_size\"\\s*:\\s*\"([^\"]+)\".*?\"quantization_level\"\\s*:\\s*\"([^\"]+)\""
            val regex = modelPattern.toRegex(RegexOption.DOT_MATCHES_ALL)
            
            regex.findAll(json).forEach { matchResult ->
                val name = matchResult.groupValues[1]
                val sizeStr = matchResult.groupValues[2]
                val parameterSize = matchResult.groupValues[3]
                val quantizationLevel = matchResult.groupValues[4]
                
                val sizeBytes = sizeStr.toLongOrNull() ?: 0L
                models.add(ModelInfo(
                    name = name,
                    size = formatSize(sizeBytes),
                    parameterSize = parameterSize,
                    quantizationLevel = quantizationLevel,
                    isDownloaded = true,
                    downloadSize = formatSize(sizeBytes)
                ))
            }
            
            models
        } catch (e: Exception) {
            println("‚ùå Error parsing downloaded models: ${e.message}")
            emptyList()
        }
    }
    
    private fun parseAllModels(json: String): List<ModelInfo> {
        return try {
            val models = mutableListOf<ModelInfo>()
            
            // Use a simpler approach - just return a hardcoded list for now
            val hardcodedModels = listOf(
                ModelInfo("llama3.1:8b", "4.6GB", "8B", "Q4_K_M", false, ""),
                ModelInfo("llama3.1:70b", "39GB", "70B", "Q4_K_M", false, ""),
                ModelInfo("llama3.2:1b", "1.2GB", "1.2B", "Q8_0", false, ""),
                ModelInfo("llama3.2:3b", "1.9GB", "3.2B", "Q4_K_M", false, ""),
                ModelInfo("llama3.2:11b", "6.2GB", "11B", "Q4_K_M", false, ""),
                ModelInfo("llama3.2:90b", "50GB", "90B", "Q4_K_M", false, ""),
                ModelInfo("codellama:7b", "3.6GB", "7B", "Q4_0", false, ""),
                ModelInfo("codellama:13b", "7.3GB", "13B", "Q4_0", false, ""),
                ModelInfo("codellama:34b", "19GB", "34B", "Q4_0", false, ""),
                ModelInfo("mistral:7b", "4.1GB", "7B", "Q4_K_M", false, ""),
                ModelInfo("mixtral:8x7b", "45GB", "8x7B", "Q4_K_M", false, ""),
                ModelInfo("neural-chat:7b", "4.1GB", "7B", "Q4_K_M", false, ""),
                ModelInfo("starling-lm:7b", "4.1GB", "7B", "Q4_K_M", false, ""),
                ModelInfo("openchat:7b", "4.1GB", "7B", "Q4_K_M", false, ""),
                ModelInfo("gemma:2b", "1.6GB", "2B", "Q4_K_M", false, ""),
                ModelInfo("gemma:7b", "4.8GB", "7B", "Q4_K_M", false, ""),
                ModelInfo("phi3:mini", "2.3GB", "3.8B", "Q4_K_M", false, ""),
                ModelInfo("phi3:medium", "7.4GB", "14B", "Q4_K_M", false, ""),
                ModelInfo("qwen2.5:7b", "4.4GB", "7B", "Q4_K_M", false, ""),
                ModelInfo("qwen2.5:14b", "8.6GB", "14B", "Q4_K_M", false, ""),
                ModelInfo("qwen2.5:32b", "19GB", "32B", "Q4_K_M", false, ""),
                ModelInfo("deepseek-coder:6.7b", "3.8GB", "6.7B", "Q4_K_M", false, ""),
                ModelInfo("deepseek-coder:33b", "19GB", "33B", "Q4_K_M", false, ""),
                ModelInfo("wizardcoder:7b", "4.1GB", "7B", "Q4_K_M", false, ""),
                ModelInfo("wizardcoder:13b", "7.3GB", "13B", "Q4_K_M", false, ""),
                ModelInfo("wizardcoder:34b", "19GB", "34B", "Q4_K_M", false, "")
            )
            
            println("üîç Using hardcoded list of ${hardcodedModels.size} available models")
            hardcodedModels
        } catch (e: Exception) {
            println("‚ùå Error parsing all models: ${e.message}")
            e.printStackTrace()
            emptyList()
        }
    }
    
    private fun extractJsonValue(json: String, key: String): String? {
        // First try to match string values (with quotes)
        val stringPattern = "\"$key\"\\s*:\\s*\"([^\"]+)\""
        val stringRegex = stringPattern.toRegex()
        val stringMatch = stringRegex.find(json)?.groupValues?.get(1)
        if (stringMatch != null) return stringMatch
        
        // Then try to match numeric values (without quotes)
        val numberPattern = "\"$key\"\\s*:\\s*(\\d+)"
        val numberRegex = numberPattern.toRegex()
        return numberRegex.find(json)?.groupValues?.get(1)
    }
    
    private fun formatSize(bytes: Long): String {
        return when {
            bytes >= 1_000_000_000 -> "${bytes / 1_000_000_000}GB"
            bytes >= 1_000_000 -> "${bytes / 1_000_000}MB"
            bytes >= 1_000 -> "${bytes / 1_000}KB"
            else -> "${bytes}B"
        }
    }
}

fun main(args: Array<String>) = runBlocking {
    startWebServer()
}

fun runBlocking(block: suspend () -> Unit) {
    kotlinx.coroutines.runBlocking { block() }
}

suspend fun startWebServer() {
    println("üöÄ Starting Docker AI Agent Web Server...")
    println("üê≥ Using Ollama running in Docker container")
    println("üåê Web interface will be available at: http://localhost:8080")
    println("‚îÄ".repeat(60))
    
    // Create Docker Ollama agent with improved timeout settings and memory
    val dockerOllamaExecutor = DockerOllamaExecutor(
        baseUrl = "http://localhost:11434", // Docker container address
        model = "llama3.1:8b",
        maxRetries = 3,
        requestTimeout = Duration.ofMinutes(5), // 5 minutes for complex responses
        connectTimeout = Duration.ofSeconds(60)  // 60 seconds for connection
    )
    
    val dockerAgent = DockerAIAgent(
        systemPrompt = "You are a helpful, intelligent, and friendly AI assistant running in a Docker container. You provide clear, accurate, and engaging responses. You can help with various tasks including coding, writing, analysis, and general knowledge questions. Remember the conversation context and refer back to previous messages when relevant. You can also help with file operations and document processing.",
        modelName = "Llama3.1:8b (Docker)",
        executor = dockerOllamaExecutor,
        maxMemorySize = 10 // Keep last 10 messages for context
    )

    // Health check before starting
    println("\nüîç Checking Docker Ollama health...")
    val healthStatus = dockerAgent.checkHealth()
    println("üìã $healthStatus")
    
    if (!healthStatus.contains("healthy")) {
        println("‚ö†Ô∏è Warning: Docker Ollama may not be ready. Continuing anyway...")
    }

    // Show available models
    println("\nüîç Checking available models in Docker Ollama...")
    kotlinx.coroutines.delay(1000) // Small delay to avoid race condition
    val modelsInfo = dockerAgent.listModels()
    println("üìã $modelsInfo")

    // Start web server
    println("\nüåê Starting web server on port 8080...")
    val webServer = SimpleWebServer(dockerAgent)
    webServer.start(8080)
}

fun startInteractiveChat() = runBlocking {
    println("üöÄ Starting Docker-based AI Agent Interactive Chat System with Memory & File I/O...")
    println("üê≥ Using Ollama running in Docker container")
    println("üß† Conversation memory enabled (last 10 messages)")
    println("üìÅ File I/O capabilities enabled")
    println("üìù Available commands: 'exit', 'quit', 'clear', 'help', 'models', 'health', 'memory', 'save', 'read', 'write', 'list', 'pull', 'delete', 'current', 'switch', 'stats', 'export', 'analytics', 'reset', 'monitor', 'performance', 'health-check'")
    println("üí¨ Start chatting with your Docker-based AI agent!")
    println("‚îÄ".repeat(60))
    
    // Create Docker Ollama agent with improved timeout settings and memory
    val dockerOllamaExecutor = DockerOllamaExecutor(
        baseUrl = "http://localhost:11434", // Docker container address
        model = "llama3.1:8b",
        maxRetries = 3,
        requestTimeout = Duration.ofMinutes(5), // 5 minutes for complex responses
        connectTimeout = Duration.ofSeconds(60)  // 60 seconds for connection
    )
    
    val dockerAgent = DockerAIAgent(
        systemPrompt = "You are a helpful, intelligent, and friendly AI assistant running in a Docker container. You provide clear, accurate, and engaging responses. You can help with various tasks including coding, writing, analysis, and general knowledge questions. Remember the conversation context and refer back to previous messages when relevant. You can also help with file operations and document processing.",
        modelName = "Llama3.1:8b (Docker)",
        executor = dockerOllamaExecutor,
        maxMemorySize = 10 // Keep last 10 messages for context
    )

    // Health check before starting
    println("\nüîç Checking Docker Ollama health...")
    val healthStatus = dockerAgent.checkHealth()
    println("üìã $healthStatus")
    
    if (!healthStatus.contains("healthy")) {
        println("‚ö†Ô∏è Warning: Docker Ollama may not be ready. Continuing anyway...")
    }

    // Show available models
    println("\nüîç Checking available models in Docker Ollama...")
    kotlinx.coroutines.delay(1000) // Small delay to avoid race condition
    val modelsInfo = dockerAgent.listModels()
    println("üìã $modelsInfo")

    // Interactive chat loop
    println("\nüéØ Interactive chat started! Type your messages below:")
    println("üí° Tip: Type 'help' for available commands")
    println("üß† Memory: ${dockerAgent.getMemorySummary()}")
    println("üìÅ File I/O: Ready for file operations")
    println("‚îÄ".repeat(60))
    
    val scanner = java.util.Scanner(System.`in`)
    var conversationCount = 0
    
    while (true) {
        try {
            print("\nüë§ You: ")
            val userInput = scanner.nextLine().trim()
            
            when {
                userInput.isEmpty() -> continue
                userInput.lowercase() in listOf("exit", "quit") -> {
                    println("üëã Goodbye! Thanks for chatting with the Docker AI agent!")
                    break
                }
                userInput.lowercase() == "help" -> {
                    println("üìö Available commands:")
                    println("  ‚Ä¢ 'exit' or 'quit' - End the chat session")
                    println("  ‚Ä¢ 'clear' - Clear the conversation memory")
                    println("  ‚Ä¢ 'help' - Show this help message")
                    println("  ‚Ä¢ 'models' - List available AI models")
                    println("  ‚Ä¢ 'health' - Check Docker Ollama health")
                    println("  ‚Ä¢ 'memory' - Show conversation memory status")
                    println("  ‚Ä¢ 'current' - Show current model")
                    println("  ‚Ä¢ 'pull <model_name>' - Download a new model")
                    println("  ‚Ä¢ 'delete <model_name>' - Delete a model")
                    println("  ‚Ä¢ 'switch <model_name>' - Switch to a different model")
                    println("  ‚Ä¢ 'stats' - Show conversation statistics")
                    println("  ‚Ä¢ 'export [format]' - Export conversation (json/txt/csv)")
                    println("  ‚Ä¢ 'analytics' - Show detailed conversation analytics")
                    println("  ‚Ä¢ 'reset' - Reset session and clear all data")
                    println("  ‚Ä¢ 'monitor' - Show system monitoring dashboard")
                    println("  ‚Ä¢ 'performance' - Show performance metrics")
                    println("  ‚Ä¢ 'health-check' - Comprehensive system health check")
                    println("  ‚Ä¢ 'save [filename]' - Save conversation to file")
                    println("  ‚Ä¢ 'read <filename>' - Read file content")
                    println("  ‚Ä¢ 'write <filename> <content>' - Write content to file")
                    println("  ‚Ä¢ 'list [directory]' - List files in directory")
                    println("  ‚Ä¢ Any other text - Chat with the AI agent (with memory!)")
                    continue
                }
                userInput.lowercase() == "clear" -> {
                    dockerAgent.clearMemory()
                    println("üßπ Conversation memory cleared!")
                    conversationCount = 0
                    continue
                }
                userInput.lowercase() == "memory" -> {
                    println("üß† Memory Status:")
                    println("  ‚Ä¢ ${dockerAgent.getMemorySummary()}")
                    println("  ‚Ä¢ Memory size: ${dockerAgent.getMemorySize()}/10")
                    continue
                }
                userInput.lowercase() == "models" -> {
                    println("üîç Checking available models...")
                    val models = dockerAgent.listModels()
                    println("üìã $models")
                    continue
                }
                userInput.lowercase() == "health" -> {
                    println("üîç Checking Docker Ollama health...")
                    val health = dockerAgent.checkHealth()
                    println("üìã $health")
                    continue
                }
                userInput.lowercase() == "current" -> {
                    println("ü§ñ Current model: ${dockerAgent.getCurrentModel()}")
                    continue
                }
                userInput.lowercase().startsWith("pull ") -> {
                    val modelName = userInput.substringAfter("pull ").trim()
                    if (modelName.isNotEmpty()) {
                        println("üì• Pulling model: $modelName")
                        val result = dockerAgent.pullModel(modelName)
                        println("üìã $result")
                    } else {
                        println("‚ùå Please specify a model name: pull <model_name>")
                    }
                    continue
                }
                userInput.lowercase().startsWith("delete ") -> {
                    val modelName = userInput.substringAfter("delete ").trim()
                    if (modelName.isNotEmpty()) {
                        println("üóëÔ∏è Deleting model: $modelName")
                        val result = dockerAgent.deleteModel(modelName)
                        println("üìã $result")
                    } else {
                        println("‚ùå Please specify a model name: delete <model_name>")
                    }
                    continue
                }
                userInput.lowercase().startsWith("switch ") -> {
                    val modelName = userInput.substringAfter("switch ").trim()
                    if (modelName.isNotEmpty()) {
                        println("üîÑ Switching to model: $modelName")
                        val result = dockerAgent.switchModel(modelName)
                        println("üìã $result")
                        if (result.contains("successfully")) {
                            println("ü§ñ New current model: ${dockerAgent.getCurrentModel()}")
                        }
                    } else {
                        println("‚ùå Please specify a model name: switch <model_name>")
                    }
                    continue
                }
                userInput.lowercase() == "stats" -> {
                    println("üìä Conversation Statistics:")
                    val stats = dockerAgent.getConversationStats()
                    stats.forEach { (key, value) ->
                        when (key) {
                            "sessionDuration" -> println("  ‚Ä¢ Session Duration: ${value}ms (${(value as Long) / 1000}s)")
                            "averageResponseTime" -> println("  ‚Ä¢ Average Response Time: ${String.format("%.2f", value)}ms")
                            else -> println("  ‚Ä¢ ${key.replaceFirstChar { it.uppercase() }}: $value")
                        }
                    }
                    continue
                }
                userInput.lowercase().startsWith("export") -> {
                    val format = userInput.substringAfter("export").trim().ifEmpty { "json" }
                    println("üì§ Exporting conversation in $format format...")
                    val result = dockerAgent.exportConversation(format)
                    println("üìã $result")
                    continue
                }
                userInput.lowercase() == "analytics" -> {
                    println("üìà Conversation Analytics:")
                    val analytics = dockerAgent.getConversationAnalytics()
                    if (analytics.containsKey("message")) {
                        println("  ‚Ä¢ ${analytics["message"]}")
                    } else {
                        analytics.forEach { (key, value) ->
                            when (key) {
                                "sessionDuration" -> println("  ‚Ä¢ Session Duration: ${String.format("%.2f", value)}s")
                                "averageResponseTime" -> println("  ‚Ä¢ Average Response Time: ${String.format("%.2f", value)}ms")
                                "averageMessageLength" -> println("  ‚Ä¢ Average Message Length: ${String.format("%.2f", value)} characters")
                                else -> println("  ‚Ä¢ ${key.replaceFirstChar { it.uppercase() }}: $value")
                            }
                        }
                    }
                    continue
                }
                userInput.lowercase() == "reset" -> {
                    println("üîÑ Resetting session...")
                    dockerAgent.resetSession()
                    println("‚úÖ Session reset complete!")
                    conversationCount = 0
                    continue
                }
                userInput.lowercase() == "monitor" -> {
                    println("üìä System Monitoring Dashboard:")
                    val metrics = dockerAgent.getSystemMetrics()
                    println("  üîß CPU Usage: ${String.format("%.1f", metrics.cpuUsage)}%")
                    println("  üíæ Memory Usage: ${String.format("%.1f", metrics.memoryUsage)}%")
                    println("  üíø Disk Usage: ${String.format("%.1f", metrics.diskUsage)}%")
                    println("  üåê Network Latency: ${metrics.networkLatency}ms")
                    println("  üîó Active Connections: ${metrics.activeConnections}")
                    println("  ‚è±Ô∏è Uptime: ${metrics.uptime / 1000}s")
                    println("  üìÖ Last Updated: ${java.time.Instant.ofEpochMilli(metrics.lastUpdated)}")
                    continue
                }
                userInput.lowercase() == "performance" -> {
                    println("‚ö° Performance Metrics:")
                    val performance = dockerAgent.getPerformanceMetrics()
                    println("  üìà Average Response Time: ${String.format("%.2f", performance.averageResponseTime)}ms")
                    println("  üìä Total Requests: ${performance.totalRequests}")
                    println("  ‚úÖ Successful Requests: ${performance.successfulRequests}")
                    println("  ‚ùå Failed Requests: ${performance.failedRequests}")
                    println("  üöÄ Throughput: ${String.format("%.2f", performance.throughput)} requests/min")
                    println("  ‚ö†Ô∏è Error Rate: ${String.format("%.2f", performance.errorRate)}%")
                    println("  üèÜ Peak Response Time: ${performance.peakResponseTime}ms")
                    println("  üéØ Min Response Time: ${performance.minResponseTime}ms")
                    continue
                }
                userInput.lowercase() == "health-check" -> {
                    println("üè• Comprehensive System Health Check:")
                    val health = dockerAgent.getSystemHealth()
                    println("  üìä Status: ${health["status"]}")
                    
                    val systemMetrics = health["systemMetrics"] as DockerAIAgent.SystemMetrics
                    println("  üîß System Metrics:")
                    println("    ‚Ä¢ CPU Usage: ${String.format("%.1f", systemMetrics.cpuUsage)}%")
                    println("    ‚Ä¢ Memory Usage: ${String.format("%.1f", systemMetrics.memoryUsage)}%")
                    println("    ‚Ä¢ Disk Usage: ${String.format("%.1f", systemMetrics.diskUsage)}%")
                    println("    ‚Ä¢ Network Latency: ${systemMetrics.networkLatency}ms")
                    
                    val performanceMetrics = health["performanceMetrics"] as DockerAIAgent.PerformanceMetrics
                    println("  ‚ö° Performance Metrics:")
                    println("    ‚Ä¢ Average Response Time: ${String.format("%.2f", performanceMetrics.averageResponseTime)}ms")
                    println("    ‚Ä¢ Error Rate: ${String.format("%.2f", performanceMetrics.errorRate)}%")
                    println("    ‚Ä¢ Throughput: ${String.format("%.2f", performanceMetrics.throughput)} requests/min")
                    
                    val recommendations = health["recommendations"] as List<String>
                    println("  üí° Recommendations:")
                    recommendations.forEach { recommendation ->
                        println("    ‚Ä¢ $recommendation")
                    }
                    continue
                }
                userInput.lowercase().startsWith("save") -> {
                    val filename = userInput.substringAfter("save").trim()
                    val result = if (filename.isEmpty()) {
                        dockerAgent.saveConversation()
                    } else {
                        dockerAgent.saveConversation(filename)
                    }
                    println("üíæ $result")
                    continue
                }
                userInput.lowercase().startsWith("read ") -> {
                    val filename = userInput.substringAfter("read ").trim()
                    if (filename.isNotEmpty()) {
                        val result = dockerAgent.readFile(filename)
                        println("üìÑ $result")
                    } else {
                        println("‚ùå Please specify a filename: read <filename>")
                    }
                    continue
                }
                userInput.lowercase().startsWith("write ") -> {
                    val parts = userInput.substringAfter("write ").split(" ", limit = 2)
                    if (parts.size >= 2) {
                        val filename = parts[0]
                        val content = parts[1]
                        val result = dockerAgent.writeFile(filename, content)
                        println("‚úèÔ∏è $result")
                    } else {
                        println("‚ùå Please specify filename and content: write <filename> <content>")
                    }
                    continue
                }
                userInput.lowercase().startsWith("list") -> {
                    val directory = userInput.substringAfter("list").trim()
                    val result = if (directory.isEmpty()) {
                        dockerAgent.listFiles()
                    } else {
                        dockerAgent.listFiles(directory)
                    }
                    println("üìÅ $result")
                    continue
                }
                else -> {
                    conversationCount++
                    println("ü§ñ AI: Thinking...")
                    println("üß† Context: ${dockerAgent.getMemorySize()} previous messages")
                    
                    try {
                        val response = dockerAgent.run(userInput)
                        println("ü§ñ AI: $response")
                        println("üìä Conversation #$conversationCount completed successfully!")
                        println("üß† Memory: ${dockerAgent.getMemorySummary()}")
                    } catch (e: Exception) {
                        println("‚ùå Error: ${e.message}")
                        println("üîÑ You can try again or type 'help' for assistance")
                    }
                }
            }
        } catch (e: Exception) {
            if (e is java.util.NoSuchElementException) {
                println("\n‚ö†Ô∏è No input available. This might happen when running via Gradle.")
                println("üí° To use interactive chat, run the JAR file directly:")
                println("   java -jar app/build/libs/app.jar")
                break
            } else {
                println("‚ùå Unexpected error: ${e.message}")
                break
            }
        }
    }
    
    println("\nüìä Session Summary:")
    println("  ‚Ä¢ Total conversations: $conversationCount")
    println("  ‚Ä¢ Final memory: ${dockerAgent.getMemorySummary()}")
    println("  ‚Ä¢ Docker Ollama: $healthStatus")
    println("  ‚Ä¢ System: Fully Docker-based with memory & file I/O")
    println("  ‚Ä¢ Status: Session ended successfully")
}

/**
 * Represents a Large Language Model (LLM) with a specific provider, identifier, and a set of capabilities.
 *
 * @property provider The provider of the LLM, such as Google, OpenAI, or Meta.
 * @property id A unique identifier for the LLM instance. This typically represents the specific model version or name.
 * @property capabilities A list of capabilities supported by the LLM, such as temperature adjustment, tools usage, or schema-based tasks.
 */
@Serializable
data class LLModel(val provider: LLMProvider, val id: String, val capabilities: List<LLMCapability>)